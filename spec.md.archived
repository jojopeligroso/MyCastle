<!-- Keep Each Section Separate (Best Practice) Each section (e.g. 01-overview.md, 02-system-architecture.md, etc.) lives in /spec/. Your table-of-contents.md serves purely as a navigational index. -->
AI-Powered Learning Platform (MCP Architecture) – Project Specification
Overview
In today's educational landscape, there is a need for personalized learning experiences and efficient teaching tools. This project proposes an AI-powered learning platform that leverages multiple AI assistants to support different user roles: students, teachers, and administrators. Each user interacts with a conversational agent (chatbot) tailored to their role, providing personalized assistance. The system will be built using the Model Context Protocol (MCP) – an open standard that allows AI models (the Host) to securely connect with external data sources and tools (the Servers). By using MCP, our platform's AI assistants can seamlessly integrate with educational data (like course content, student information, analytics) in real-time while maintaining strict security and context isolation.
Project Objectives:
    • Personalized Student Support: Provide students with an AI tutor that can answer questions, explain concepts, and adapt to the student's learning progress and style.
    • Teacher Assistance: Equip educators with an AI assistant to help create content (e.g. quizzes, lesson plans), analyze student performance, and handle routine queries, thereby saving time for higher-value teaching tasks.
    • Administrative Insights: Enable administrators/business owners to interact with an AI agent for analytics, reporting, and content management, helping them make data-driven decisions about the educational platform.
    • Unified Platform Architecture: Develop a robust system using TypeScript and modern web technologies, ensuring web-first compatibility. The solution will initially run locally (for development/testing), use Supabase (a Postgres-backed platform) for data storage and authentication, and be designed for easy cloud deployment when needed.
By covering the full scope (not just a minimal prototype but all envisioned features), this specification details the requirements and architecture for a comprehensive multi-agent educational platform. The following sections describe the system architecture, role-specific AI capabilities, user stories, and development plans in detail.
System Architecture
Overall Architecture
The platform follows a client–host–server architecture inspired by the Model Context Protocol[1]. In our implementation, the host is the core application that orchestrates AI interactions and integrates with the user interface and database. The MCP servers are modules that provide specialized context, data, and tools for each user role. A high-level breakdown of components is:
    • User Interface (Client Application): A web-based chat interface for each role (student, teacher, admin). This front-end (likely built with React/TypeScript) allows users to log in (via Supabase Auth), then converse with their AI assistant. The UI sends user prompts to the host and displays the assistant's responses in real-time. Each user type will have a tailored interface or options (e.g. a student sees a homework help chat, a teacher sees a teaching assistant chat).
    • MCP Host (Backend Brain): The backend Node.js (TypeScript) application that manages the AI session. It uses the MCP protocol to coordinate between the language model and various MCP servers. The host is responsible for:
    • Handling user sessions and authorization (ensuring, for example, a student's AI only accesses student-specific data).
    • Aggregating context from the relevant MCP servers (depending on user role) and constructing prompts for the AI model.
    • Sending the user's query and the aggregated context to the AI LLM (Large Language Model) and retrieving the response.
    • Managing the conversation history and state (with possible storage of important context in Supabase for continuity).
    • LLM (Language Model) Service: The AI engine that generates the assistant's responses. This could be an external API (like OpenAI GPT-4) or a locally hosted model. The host communicates with the LLM, providing user prompts plus any supplemental context (from servers) in a single composite prompt. The LLM's reply is then relayed back to the user via the host. The system is designed to be model-agnostic, allowing swapping in a different model if needed (ensuring multi-model support in the future).
    • MCP Servers (Role-specific Modules): Three primary server modules correspond to our user roles: Student Assistant Server, Teacher Assistant Server, and Admin Assistant Server. Each server provides specialized data (resources), functions (tools), and prompt templates for its domain[2]. These servers run as isolated services (which could be separate processes or modules). The host launches and connects to them via MCP clients (each client maintaining a 1:1 connection to a server). This design ensures a clear separation of concerns:
    • The Student server handles student-related context (e.g. the student's profile, coursework, progress data) and provides tools relevant to a student's needs.
    • The Teacher server handles teacher-related context (e.g. class rosters, assignment data, grading tools).
    • The Admin server handles administrative context (e.g. overall usage statistics, content management functions). Each server operates independently and cannot access other servers' data directly, enforcing security boundaries as defined by MCP (the host controls any cross-server interactions).
    • Database (Supabase): Supabase will serve as the primary data store and back-end. It will hold persistent data such as user accounts and roles, student grades/progress records, course content and curriculum, and conversation logs or feedback. The MCP servers will interact with this database (likely via Supabase's API or client library) to retrieve and update information as needed. For example, the student server might fetch a student's past quiz scores from the DB as a resource for context, or the teacher server might invoke a tool to record feedback on a student's performance.
    • External Integrations (Optional Tools): The architecture allows adding external tools or APIs. For instance, a "knowledge base search" tool could call an external API to search textbooks or the web for answers; a math solver tool might call a computation API. These would be exposed via the MCP servers as tools the LLM can invoke. We will design the system to easily integrate such external services if required for advanced features (e.g. integrating a Wikipedia search for the student agent, or connecting to a school's SIS - Student Information System - if available).
Workflow Summary: When a user (say, a student) asks a question in the chat UI, the query is sent to the host. The host activates the Student MCP server for that session (if not already active), which supplies relevant context (resources like the student's current lesson or past performance) and available tools (functions like "fetch lesson content" or "solve equation"). The host combines the user's question, the context from the student server (and potentially other servers or general prompts), and sends it to the LLM. The LLM processes this, possibly calling one of the provided tools via the MCP protocol (e.g., it might call a tool to get additional data – this call is routed by the host to the appropriate server which executes the function and returns the result). Once the LLM has all necessary information and completes the response, the host receives the final answer and sends it back to the user interface to be displayed. All of this happens seamlessly in real-time, thanks to the structured MCP communication channel between the host and servers.
Technology Stack and Design Considerations
    • Language & Frameworks: The project will be implemented in TypeScript (with Node.js). TypeScript is preferred for its strong typing and excellent support for web development. Both the front-end and back-end can be written in TypeScript for consistency (for example, a React front-end and an Express or Next.js backend). Using TypeScript also aligns with building robust MCP servers, as there are community libraries and examples for MCP in TS, and it ensures our code is maintainable at scale.
    • MCP Implementation: We will utilize the official MCP specification and any available libraries. The MCP communication is based on JSON-RPC 2.0 for messaging[3][4]. We plan to use a Node.js library or SDK for MCP if available (for instance, example implementations on GitHub) or implement the protocol handlers ourselves following the spec. Each MCP server will expose its capabilities (tools, resources, prompts) upon initialization, and the host will negotiate these capabilities via the MCP client connections. The MCP host in our case will be custom-built into the backend server application – coordinating multiple clients and integrating with the LLM API.
    • Data Management: Supabase (with PostgreSQL) will store persistent data like user info, course content, and logs. Supabase also offers RESTful and real-time APIs that can be leveraged by MCP servers as needed. We might create dedicated Supabase Edge Functions that serve as API endpoints for complex operations (for example, a grading algorithm or a report generator) – these can then be invoked as tools by the MCP servers. Using Supabase as our backend means we have authentication, row-level security, and easy integration with the front-end out-of-the-box.
    • Security & Isolation: The system will enforce that each AI assistant only accesses data permitted for its role. MCP's design inherently supports this by isolating server contexts. For instance, even though the host connects to all three role-servers, if a student is logged in, the host will only query the Student server for that session. The teacher and admin servers would not be involved (unless explicitly allowed, such as an admin querying something that requires data from multiple sources). We will also use Supabase's auth to ensure the user's JWT or session token is verified on the backend before starting an AI session, confirming their role and privileges.
    • Scalability: The architecture can scale horizontally. Since each chat session is largely self-contained (host + some servers + LLM calls), we could run multiple host instances for different users or classes. The stateful aspect (conversation memory) can be kept in memory per session or persisted in Supabase for continuity. In a cloud deployment, we might containerize the host and servers, and use a scalable service (Supabase or similar) for the database and perhaps a vector store for long-term memory if needed. We avoid vendor lock-in for now – the design is cloud-agnostic (no specific AWS/GCP services, though it can be deployed to any with minimal changes; for now, local deployment and Supabase hosting are the targets).
    • Web-First Compatibility: The system is designed to be accessed via web browser, ensuring no special software needed by end users. The UI will be responsive and could be packaged as a desktop or mobile app later if needed (using technologies like Electron or React Native), but the primary target is the web app. All tools and resources exposed by servers will be used via the host and LLM, so from the UI perspective, everything happens through standard web requests (possibly using WebSockets or SSE for streaming responses from the AI to mimic real-time chat).
    • Development and Testing: During development, we will run the entire stack locally (Node server, possibly a local Supabase instance or the cloud one, and connecting to OpenAI or a local model). We'll use version control for code but not rely on GitHub Actions for CI initially (focus is on local rapid iteration). Testing will include unit tests for MCP server functions (ensuring tools return correct data), and integration tests simulating a full conversation for each role. We will also test security by verifying, for example, that a student's assistant cannot access teacher data, etc.
With this architecture in place, we can ensure that our AI assistants are well-integrated with the platform's data and each remains focused on its user's needs. The next section details each role-specific AI assistant, including what data and functions they provide to the AI model.
Role-Based AI Assistants and MCP Servers
This section describes the MCP servers we will implement for each user role – Student, Teacher, and Admin – and the capabilities of each. For clarity, each sub-section below identifies who the end-user is (the human role using that assistant), and enumerates the resources, tools, and prompt templates that the server provides via MCP[2]. In MCP terms, resources are typically read-only data or context provided to the model, tools are functions the model can call to perform actions or fetch information, and prompts are preset instructions or personas that guide the model's behavior for that role.
Student AI Assistant (Student MCP Server)
User: A student (typically a learner using the platform) will interact with this AI assistant through a chat interface. The assistant acts as a personal tutor or study companion, helping with coursework, answering questions, and providing guidance tailored to the student's progress.
Role Purpose: To provide personalized, context-aware academic support to the student. The AI should have a friendly, encouraging persona and adapt its help based on the student's performance and learning history.
Resources provided: (Read-only contextual data relevant to the student, fed into the AI's context) - Student Profile & Progress: Basic info about the student (name, grade level, enrolled courses) and their progress metrics. For example, recent quiz/exam scores, completed assignments, concepts they've struggled with, etc. This can be a summary or data fetched from the LMS database (Supabase) – e.g., "Student X has mastered topics A, B, but is struggling with topic C" or a record of their last activity. - Current Lesson or Materials: Details of what the student is currently studying. If the student is in the middle of a lesson or has a current homework assignment, the server can provide key details (e.g., topic of the assignment, due date, content of the lesson). This ensures the AI's answers stay on-topic and relevant to what the student should be focusing on. - Knowledge Base Excerpts: Relevant reference content, such as excerpts from textbooks or course notes. For instance, if the student asks about a concept, the server might supply the definition from their textbook or a summary from class notes as context. (This might be implemented by querying a stored content repository for the keywords of the question – essentially giving the AI a "open book" to look at for accuracy). - Conversation History Highlights: While the AI model will have short-term memory of the conversation, the student server might also store or retrieve important past interactions (e.g., "Earlier in this session, the student solved 2 problems on this topic with hints."). This helps maintain continuity in tutoring sessions, especially if a session is resumed after a break (data persisted in the DB).
Tools available: (Functions the AI can call via MCP to perform certain tasks or get live data) - searchCourseContent(query) – Search the course materials or knowledge base for a given query. This tool could scan the student's textbook, lecture slides, or an FAQ for relevant information. For example, if the student asks a question that might be answered in the course content, the AI can call this tool to get a snippet of that content[2]. - getPracticeQuiz(topic) – Generate a short practice quiz on a specified topic. This tool might interact with a question bank or algorithm to pull 3-5 practice questions (and answers) for the student. The AI can then present these to the student for additional practice. - checkSolution(answer, problemId) – Evaluate a student's answer for a specific problem. If the student uses the assistant to check their work ("I think the answer is 42, is that right?"), the AI can call this tool to verify the answer against the solution key in the database and return feedback. - summarizePerformance() – Fetch a brief summary of the student's performance across recent activities. For instance, "You've gotten 80% of algebra questions correct this week, an improvement from last week's 70%." This can help the AI give encouraging or remedial feedback. (This tool would aggregate data from DB – assignments, quiz scores, etc. – possibly using a Supabase function or a query). - (Planned) scheduleStudyReminder(time, topic) – Schedule a reminder for the student to study a topic at a certain time. This might interface with a calendar or notification system. (If notifications are out of scope for now, this can be a placeholder that records the intent in the DB, which could later be used to send an email or alert).
(The above tools are examples; more can be added as needed. Each tool will be implemented as a function on the server. When the LLM decides it needs to use one, it will issue a JSON-RPC call via MCP which the host routes to the student server, executing the function and returning the result to the LLM.)
Prompt & behavior: The student's AI assistant will have a prompt template establishing its persona as a helpful tutor. For example, a system prompt might be: "You are a friendly AI tutor assisting a student. Explain concepts clearly and step-by-step. Encourage the student and adjust your explanations to their level of understanding. If the student is wrong, gently correct them and provide hints." This prompt template is provided by the Student MCP server when the session starts (as part of the server's advertised capabilities). It ensures the model's tone and approach are appropriate for a student audience. Additionally, the server might have specialized prompt insertions, such as instructing the AI to use simpler language if the student is younger or to reference examples from class notes if relevant.
Teacher AI Assistant (Teacher MCP Server)
User: A teacher or educator using the platform. This could be a classroom teacher or tutor who wants help in generating materials, analyzing class data, or answering student inquiries. The teacher interacts via a chat interface that acts as an AI teaching assistant.
Role Purpose: To support teachers in instructional design, grading, and student engagement. The AI can handle time-consuming tasks like creating resources, give insights into student performance, and even suggest teaching strategies, enabling teachers to focus on direct student interaction.
Resources provided: - Class Roster & Student Data: Information about the teacher's class or classes. For example, a list of students, their overall performance or grades, attendance records, etc. This helps the AI answer questions like "Who hasn't turned in Assignment 3?" or provide insights like "Overall, the class average on the last test was 75%." - Curriculum and Standards: The course syllabus, learning objectives, and relevant academic standards or benchmarks. With this, the AI can ensure that any content it generates (like quiz questions or lesson plans) aligns with the required curriculum and standards the teacher is following. - Content Library: Access to the repository of teaching materials – past lesson plans, slide decks, quiz questions, and reference materials. The AI can pull from this when asked to create new content (so that it can reuse or adapt existing material) or when providing examples. For instance, if the teacher asks, "Generate a worksheet on topic X," the AI can see what was covered about X in previous materials. - Analytics & Performance Reports: Collated data on class performance, such as grade distributions, common areas of difficulty, and historical trends. This might be presented as summary statistics or charts (the server could provide textual summaries as resources). E.g., "Only 60% of students passed the last quiz on Topic Y" or "Students improved by an average of 15% after the revision session." These insights help the AI give informed answers to a teacher's questions about how the class is doing.
Tools available: - generateQuiz(topic, difficulty, numberOfQuestions) – Create a set of quiz questions (and answers) for a given topic. The teacher can specify difficulty level and how many questions. The AI uses this tool to produce draft questions which the teacher can then review. The tool might use a combination of template questions in the database and generative capabilities to formulate new ones. - createLessonPlan(topic, duration) – Draft a lesson plan outline. If a teacher asks for help planning a lesson on a certain topic for e.g. a 45-minute class, the AI can call this tool. It might return a structured outline (introduction, main activities, conclusion, etc.) possibly referencing appropriate materials from the content library. The tool could incorporate known best practices (like interactive activity suggestions if duration > 30 min). - gradeAssignment(assignmentId) – Automatically grade an assignment or quiz based on a key. This tool could take an assignment ID (or the actual student submissions in context) and return a grading report or even directly populate grades. More realistically, the AI could use this tool for one-off checks (like "Grade this one response according to rubric"). This may interface with an auto-grader function or simply fetch an answer key and compare answers. - analyzePerformance(studentId?) – Provide a detailed analysis of performance. If invoked with a specific student, it returns a report on that student (strengths, weaknesses, progress over time). If no student specified, it analyzes the class as a whole. This helps the AI answer questions like "How is John Doe doing in class?" or "What concepts are most students struggling with?" by giving concrete data. - sendAnnouncement(message) – Post an announcement to students (e.g., via the platform or email). If a teacher composes a message or asks the AI to remind students of something, the AI can call this tool to actually dispatch that announcement through the system. It might create a record in the database that triggers a notification to all students in the class. (This would be carefully authorized – likely only certain predefined channels so the AI doesn't spam inadvertently).
Prompt & behavior: The teacher's assistant prompt will cast the AI as a knowledgeable co-teacher or assistant. For example: "You are a teaching assistant AI helping an educator. Provide clear, concise information. When asked to create content, format it clearly and align with educational standards. Offer constructive suggestions for improving student outcomes. Be professional and supportive." This ensures the AI communicates at a peer level with the teacher, not too simplistic or tutoring in tone. It can also be instructed to cite data when giving analytics (e.g., "Based on the data, 5 out of 20 students..."). The teacher's AI might also have a style of acknowledging the teacher's expertise (for instance, phrases like "Perhaps you could try [strategy], since last week's quiz results showed..."). The prompt template will be provided by the Teacher MCP server, possibly along with some examples of how to format outputs like lesson plans or quiz questions for consistency.
Admin AI Assistant (Admin MCP Server)
User: An administrator or business owner of the platform – e.g., a school administrator, a content manager, or an executive at the company offering this learning platform. This user is interested in high-level insights, configuration, and maintenance rather than day-to-day teaching or learning tasks.
Role Purpose: To assist in making data-driven decisions about the platform and managing content/users. The AI acts as an analytical assistant and management aide, able to pull up statistics, generate reports, and help configure the system.
Resources provided: - Platform Usage Statistics: Data on user counts, active users, session lengths, etc. For example, total number of students on the platform, average daily active users, most popular courses, etc. These stats help answer questions like "How many active users did we have this month?" or "Which course has the highest engagement?". - Performance & Outcome Metrics: Aggregated data about student outcomes (perhaps anonymized or high-level). E.g., average improvement of students over time, success rates of certain courses, feedback ratings. If the admin is looking at effectiveness, the AI can use this data. - Content Repository Info: Information about the library of courses and content available. This could be a list of all courses, their completion rates, their ratings, etc. Useful for queries like "What new content should we develop?" or "Which course has the lowest completion rate (and might need improvement)?". - System Config & Logs (limited): Data about system status – e.g., last backup time, error logs summary, or AI usage stats (how often the AI is invoked, average response time). This can be useful for an admin monitoring technical performance or cost ("How many AI tokens are we using per day?"). We will be cautious to provide only non-sensitive logs as resources, focusing on summary info that an admin would need.
Tools available: - generateReport(type, timeframe) – Produce a formatted report of a given type (e.g., "usage", "performance", "financial") for a specified timeframe. For instance, an admin can ask "Generate a monthly usage report for October" and the AI will call generateReport("usage","Oct 2025") which compiles key metrics into a summary (possibly with tables or bullet points). Types might include usage, user growth, or learning outcomes. The tool will gather relevant data from the database and return a summary that the AI can present. - manageContent(action, contentId, params) – Perform a content management action. This is a general tool that could handle tasks like publishing or unpublishing a course, updating course metadata, or flagging content. For example, if the admin says "Unpublish the course ID 123", the AI calls manageContent("unpublish", 123). This tool would then update the database to mark that course as unpublished. (We will ensure proper validation so only authorized actions happen.) - userManagement(action, userId) – Manage user accounts (within allowed scope). E.g., reset a teacher's password, deactivate a student account, promote a user to teacher, etc. The AI could use this to fulfill admin commands like "reset Teacher Bob's password" by calling userManagement("resetPassword", bobId). Security is critical here; this tool will likely have safeguards and confirmations (the AI might ask the admin user for confirmation if a destructive action is attempted). - fetchFeedback(query, filter) – Retrieve user feedback or support tickets matching certain criteria. If the platform collects feedback from students/teachers or has support tickets, the admin might ask "Show me feedback about the new AI tutor feature." This tool would search the feedback database for relevant comments and return a summarized list. The AI can then parse or quote these in the response. This helps the admin gauge user sentiment or identify common issues. - systemStatusCheck(component) – Provides status of a system component (e.g., "database", "LLM API", "MCP servers"). If an admin asks something like "Is everything running smoothly?", the AI can call this tool, which might run some health checks or fetch uptime info. It would return something like "All systems operational" or "The LLM API has 2 errors in the last hour" that the AI can report. (This overlaps with dev-ops, which might be beyond initial scope, but is a possible tool for an admin concerned with system reliability).
Prompt & behavior: The admin's AI assistant will have a professional, succinct tone, similar to a business analyst or an IT support assistant, depending on query. A sample system prompt: "You are an AI administrative assistant for an educational platform. You provide analytical insights, reports, and help manage the system. Be concise and factual in your responses. Use a formal tone. When giving statistics or reports, format them clearly (use lists or tables if helpful)." This ensures the AI provides value to the admin without unnecessary tutoring or conversational fluff. It may also be instructed to double-check before executing critical actions (for instance, if asked to delete a course, it might respond with "Are you sure? Please confirm." rather than doing it immediately). The prompt template guides the AI to always consider the overarching business goal (e.g., improving user engagement, ensuring system stability) when answering questions or making suggestions to the admin.
Common and Future Capabilities
(This subsection covers any additional servers or tools that span multiple roles, and notes on extensibility.)
    • Common Knowledge/Base AI: We might have a shared general knowledge base or reasoning capability that all assistants can use (for example, a general encyclopedia or math solver). Rather than duplicating this in each server, one approach is to have a separate MCP server for general tools (like a "General Knowledge Server" providing a web search or calculator tool) that could be attached to sessions as needed. Initially, we will focus on the three primary servers. However, the architecture allows adding more servers seamlessly[1]. For instance, in the future we could add a "Mentor AI" server that students and teachers can both consult for mentorship or a "Content Recommender" server that suggests new courses to students.
    • Cross-Role Interaction: In normal operation, each user's AI is isolated to their role's context. However, there may be scenarios where limited cross-role communication is useful. For example, if a student asks the tutor AI "Can I get my overall grade in the class?", the student AI might not have that data directly, but the teacher server does. The host could facilitate a query from the student AI to the teacher server in a controlled manner (since the teacher server's data for that class might be accessible to that student's context in a limited form). To handle this, we can allow the host to fetch specific information from another server if appropriate permissions are met (in this example, a student requesting their own grade is fine). Under MCP, direct server-to-server talk doesn't happen – the host would act as intermediary, pulling a resource from the teacher server and including it in the student's context. This will be designed carefully and logged for transparency.
    • Privacy and Consent: Especially in an educational setting, privacy is crucial. We will implement consent rules for data exposure. For instance, the admin AI can access aggregate student data but not individual student's personal information unless it's anonymized or the admin has that privilege. Similarly, a teacher's assistant should not reveal one student's info to another student. MCP servers by design expose only what they are coded to, and the host will ensure only relevant servers are connected per session, serving as an extra gatekeeper.
    • Multi-Tenancy: If the platform is used by multiple institutions or classes, our servers will include context of which class or tenant they are serving to partition the data. For example, the teacher server will ensure it only fetches data for classes that the authenticated teacher user teaches. Supabase row-level security policies will be leveraged to enforce this at the database level too.
    • Extensibility: The MCP approach means new capabilities can be added by introducing new servers or enhancing existing ones. For instance, adding a "Parent AI Assistant" (if we ever wanted a parent-facing chatbot) could be done by creating a new MCP server with relevant tools (like viewing their child's progress) and hooking it into the same host and UI framework. This extensibility is a key advantage of following the MCP standard – our host could potentially connect to any compliant server in the future with minimal changes[5][6].
With each role-specific assistant defined, the functional requirements of the system become clearer. We will now translate these into user stories and epics to guide development and ensure we meet all user needs.
User Stories and Epics
To capture the full scope of features for this project, we outline key epics (major feature areas) and detailed user stories for each epic. User stories are written from the perspective of end-users (students, teachers, admins) or occasionally other stakeholders, describing what they need and why. These will serve as the project requirements, ensuring we implement functionality that delivers value to each user type.
Epic 1: Personalized Student Tutoring
Description: Features that enable students to receive personalized help and engage in effective learning through the AI tutor.
    • Story 1.1: As a student, I want to ask the AI tutor questions in natural language and get clear, step-by-step explanations, so that I can understand difficult concepts on my own time.
    • Story 1.2: As a student, I want the AI tutor to provide hints rather than full solutions for homework problems, so that I am guided to find the answer myself and truly learn the material.
    • Story 1.3: As a student, I want the AI tutor to remember my past questions and mistakes, so that it can tailor its guidance (e.g., not repeat explanations I already know, and focus on areas I struggle).
    • Story 1.4: As a student, I want to take practice quizzes generated by the AI on topics I choose, so that I can test my knowledge and get instant feedback in preparation for exams.
    • Story 1.5: As a student, I want to receive encouragement and study tips from the AI if I'm doing poorly, so that I stay motivated and know how to improve (for example, suggesting study schedule or resources).
    • Story 1.6: As a student, I want to be able to ask the AI tutor for examples or analogies, so that complex ideas are easier to grasp with real-world context.
(Epic 1 focuses on the student's learning experience. The above stories ensure the AI is helpful, adaptive, and supportive. Success criteria for these would include high student satisfaction and measurable improvement in student performance over time.)
Epic 2: Teacher's Assistant and Classroom Management
Description: Features that assist teachers with content creation, grading, and understanding student needs via the AI assistant.
    • Story 2.1: As a teacher, I want the AI to generate quiz questions, worksheets, or exam problems on a given topic, so that I can quickly prepare assessment materials without starting from scratch.
    • Story 2.2: As a teacher, I want to ask the AI for a lesson plan outline or activity ideas, so that I can enrich my classes with proven pedagogical strategies (especially for topics I haven't taught before).
    • Story 2.3: As a teacher, I want the AI to grade objective-type quizzes automatically, so that I save time on grading and can focus on subjective assessments and feedback.
    • Story 2.4: As a teacher, I want to get a summary of which topics the class found most difficult (based on performance data), so that I can plan a review session or adjust my teaching on those topics.
    • Story 2.5: As a teacher, I want to be notified or informed if a particular student is consistently struggling, so that I can intervene and provide help to that student proactively. (This might involve the AI analyzing the student's interactions or scores and alerting the teacher via the assistant.)
    • Story 2.6: As a teacher, I want to use the AI to send a quick announcement or reminder to all my students, so that important updates (like "Project due tomorrow" or "Exam on Monday") are effectively communicated.
    • Story 2.7: As a teacher, I want the AI to answer routine student questions on the class forum or chat (with my oversight), so that common queries (e.g., due dates, instructions clarification) are addressed quickly and I only handle the more complex or personal questions.
(Epic 2 ensures the teacher's AI assistant significantly reduces administrative burden and provides insights for better teaching. It should lead to faster content prep and improved responsiveness to student needs.)
Epic 3: Administrative Oversight and Insights
Description: Features that empower an administrator to monitor and improve the platform through the AI assistant.
    • Story 3.1: As an admin, I want to see usage statistics (daily active users, new sign-ups, etc.) in a report, so that I can gauge the platform's growth and engagement.
    • Story 3.2: As an admin, I want the AI to provide me with a summary of student performance across all courses, so that I know if our platform is effectively helping learners (for example, course completion rates or average improvements).
    • Story 3.3: As an admin, I want to be able to ask the AI for financial metrics (if applicable, e.g., subscription numbers or revenue) securely, so that I can make business decisions or report to stakeholders.
    • Story 3.4: As an admin, I want to easily manage content (add/remove courses, update info) via a conversation with the AI, so that I can perform content updates without navigating a complex CMS. (e.g., "AI, unpublish the outdated course on ABC" and it's done with confirmation.)
    • Story 3.5: As an admin, I want to review feedback from users categorized by topic, so that I can identify common praise or complaints and prioritize improvements.
    • Story 3.6: As an admin, I want to ensure the AI is behaving well for users and not giving wrong or inappropriate answers, so that we maintain trust and effectiveness. (This might involve asking the admin AI to audit interactions or highlight any flagged content. Although challenging, an admin could query "Have any AI responses been flagged by users this week?" and the system would provide relevant info.)
    • Story 3.7: As an admin, I want to use the AI to perform user account actions like resetting passwords or changing roles in a natural way, so that administrative support tasks can be done quickly (with security verifications).
(Epic 3 focuses on the oversight and control aspects. Implementing these will give administrators a powerful interface to manage the platform and glean insights without needing advanced technical know-how.)
Epic 4: System Architecture & Development Efficiency (Internal)
Description: (This epic is about how we build the system efficiently using AI assistance and robust architecture. It's not directly user-facing but crucial for delivering the above features reliably.)
    • Story 4.1: As a developer (project team), I want to use the MCP standard to integrate the AI with our data sources, so that we have a modular and secure system where each component can be developed and updated independently.[1]
    • Story 4.2: As a developer, I want to implement each role's capabilities as separate MCP servers in TypeScript, so that the context and tools for each role are isolated and maintainable (e.g., we can work on the teacher's tools without risking student's logic).
    • Story 4.3: As a developer, I want to easily spin up the system locally and run tests for each agent's functionality, so that I can rapidly iterate and ensure quality for each feature (using supabase's local dev and perhaps mock LLM for testing).
    • Story 4.4: As the project manager, I want to leverage an AI coding assistant (Claude or similar) for generating boilerplate code and simple functions, so that development is faster and I can focus on reviewing and integrating code rather than writing everything from scratch.
    • Story 4.5: As the project manager, I want to coordinate multiple AI "agent" workers to work on different tasks concurrently, so that we can develop features in parallel (for example, one agent writes front-end components while another writes back-end functions), speeding up delivery.
    • Story 4.6: As the project manager, I want a clear communication channel (log) of what each AI agent is doing, so that I can oversee the multi-agent development process, ensure they stay on track, and resolve any conflicts or duplication of work.
(Epic 4 highlights non-functional requirements and our development approach. It ensures the project is feasible and maintainable by using best practices and AI augmentation in the development process. We include it in the spec because it influences how we plan and implement the above features.)
These user stories collectively form the basis of our project requirements. We will ensure each story is addressed during development. Next, we outline the project planning, including how we'll execute these tasks (often with the help of AI coding agents) and how the multi-agent development will be managed.
Project Plan and Task Breakdown
In this section, we present a plan for implementing the project, including major phases, tasks, and how we'll utilize AI assistance (Cloud) in our development workflow. The plan is comprehensive to cover the entire project scope. Development will be approached iteratively, but with a clear breakdown of components to build.
Development Approach and AI-assisted Workflow
We will follow an agile, iterative development process with an emphasis on using AI to boost productivity. A specialized AI coding assistant (referred to as "Cloud") will be invoked for coding tasks whenever feasible. The idea is to treat Cloud as a member of the development team: - Routine or boilerplate coding tasks (setting up project structure, writing simple CRUD functions, converting user stories into code) will be delegated to the AI in the form of well-defined subtasks. For example, we might prompt Cloud to "Create a TypeScript class for managing MCP server connections" or "Generate a React component for the chat interface with these requirements…". - Cloud's output will then be reviewed, tested, and integrated by human developers (acting as the project managers/architects). This ensures that while we maximize speed, we maintain quality control. We expect Cloud to produce initial drafts of code which we refine. - Complex or critical sections (security-sensitive code, novel algorithms) will be written with more caution – possibly starting with Cloud's suggestions but heavily reviewed or hand-written if needed. - We will maintain a context file (the CloudMD file) that contains the project's high-level specifications and coding guidelines. This file will be provided to the AI assistant for context whenever it's given a coding task. It ensures Cloud is aware of the project architecture, naming conventions, data models, etc., reducing miscommunication. For instance, the CloudMD might summarize our data schema so the AI knows what fields exist in the student profile or how the tool APIs should look.
We will encourage parallel development by AI where possible: - Different feature tasks can be handled concurrently by separate instances of the AI (or sequentially if using one instance, but conceptually parallel). For example, one instance of Cloud works on the Student server implementation while another works on the Teacher server, as long as they don't depend on each other. - To coordinate this, a shared channel (ChannelMD) will be used to simulate agent-to-agent communication and logging. This channel is essentially a transcript where each agent (or developer) notes progress or asks questions. For example, after Cloud finishes coding a module, it might "post" in channel.md that it's completed, and another agent (or the human overseer) can pick it up for integration. If two AI agents need to interface (say one is designing an API that another will consume), they will use the channel to agree on the interface format.
Overall, this AI-augmented approach aims to speed up development while the human team focuses on architecture, integration, and polishing. Now, we break down the tasks and phases of the project:
Phase 1: Foundation Setup
Goal: Establish the core infrastructure of the project so that all subsequent features can be built on a solid base. - Task 1.1: Requirements Review and Domain Modeling – Translate user stories into a conceptual data model. Define the schema for core entities: User (with roles student/teacher/admin), Course, Lesson, Assignment, Quiz, etc. Define what data each MCP server will need (e.g., student progress structure, performance metrics structure). (Owner: Human architect; Outcome: Entity-relationship diagram, updated spec if needed.) - Task 1.2: Project Skeleton – Initialize a monorepo or structured repository (if using e.g. NX or just separate folders) with: - A backend server (Node/Express or Next.js API routes) in TypeScript. - A frontend (React/Next.js app) in TypeScript. - Basic configuration for Supabase (setting up the database and perhaps using Supabase CLI to generate types from the schema). - This will include setting up linting, formatting, and any required config files. (Owner: Cloud AI with prompt "Set up a Next.js + Node project with Typescript, including [list requirements]"; human to review.) - Task 1.3: Supabase Database Initialization – Implement the database schema in Supabase. Use Supabase SQL migration or the dashboard to create tables for Users, Courses, Enrollments (which student in which course), Assignments, etc., according to the model from Task 1.1. Also set up row-level security policies so students can only read their data, teachers their classes, etc. (Owner: Human, possibly assisted by Cloud for writing SQL.) - Task 1.4: Authentication & User Management – Leverage Supabase Auth to handle user sign-up/login. Seed the system with example users (one student, one teacher, one admin) for testing. Ensure the frontend can log users in and the backend receives a valid auth token to identify the user and role. (Owner: Cloud to scaffold auth integration, human to configure settings.) - Task 1.5: MCP Host Implementation – Build the core host logic that can manage an AI session. This includes: - Creating a SessionManager or similar class to start/stop sessions. - Implementing the logic to spawn/connect to MCP servers. Possibly use a library or spawn child processes for each server (during development, they could even run as threads). - Handling JSON-RPC message routing between the LLM and servers (for tools calls and resource subscriptions). For now, a simple loop: upon user message, gather resources from each active server, format prompt, call LLM API, then handle any tool calls in the LLM's response by invoking server functions, etc. - Manage conversation history (store last N messages or use an approach to keep context limited). (Owner: Cloud for basic structure, with human refining logic. This is a critical part, so heavy testing will follow.) - Task 1.6: LLM Integration – Write a module to interface with the chosen LLM (e.g., OpenAI API wrapper). Include configurations for model type, handling of streaming responses (if using), and error handling/retry logic. Possibly implement a stub for local dev if API not always available. (Owner: Cloud to generate boilerplate for API calls, human to insert API keys and verify.)
Milestone at end of Phase 1: We have a basic system where a user can log in and start a chat session. The host and at least one dummy MCP server (with trivial data) are running, and the AI can respond with a generic answer. For example, a logged-in student could ask "Hello" and get a response (not yet personalized since servers are not fully implemented). This confirms the end-to-end pipeline (UI → Host → LLM → back to UI) works.
Phase 2: Implement Role-Specific Servers & Features (Incremental)
In this phase, we will iteratively implement each MCP server (student, teacher, admin) along with the front-end features for that role. We'll likely tackle one role at a time to completion or alternate to keep things interesting and parallelize work.
    • Task 2.1: Student MCP Server Implementation – Develop the Student server:
    • Define its API (resources, tools) according to the spec above. In code, this means creating a TypeScript class or module with those functions (searchCourseContent, getPracticeQuiz, etc.) and a way to retrieve the needed data from Supabase. For resources, implement subscriptions or fetches for things like current lesson content or student profile (could be as simple as a function that the host calls at each query, or using MCP's resource subscription to push updates).
    • Program the prompt template for student persona and ensure it's sent to the host on initialization.
    • Ensure security: the server should verify the session's user is indeed allowed to access the specific student data (though Supabase queries inherently can be limited by RLS, we add checks as well). (Owner: Cloud can be tasked with "Implement StudentServer class with the following tools: ..." using CloudMD context of what each does. Human to fine-tune the queries and logic.)*
    • Task 2.2: Student Front-end Integration – Build out the front-end features for students:
    • Create the Student chat interface page, if not identical to a common chat component. This might involve some role-specific UI elements (e.g., a "request quiz" button that triggers a specific prompt).
    • Add a view for practice quiz results or any UI needed to display things like a quiz generated by the AI nicely (maybe as a list of questions).
    • Ensure the front-end passes the student's context (like course selection if needed) to the backend – possibly via the JWT or via an API call to set current course, etc. (Owner: Cloud to scaffold React components for chat and quiz display. Human to integrate with backend API endpoints.)*
    • Task 2.3: Teacher MCP Server Implementation – Develop the Teacher server:
    • Implement tools such as generateQuiz, createLessonPlan, etc. These may be more complex as they involve content generation. Some might directly leverage the LLM (but since the LLM is already in the loop, these tools could be implemented as code that perhaps calls a smaller model or just uses template logic – or they can call the main LLM again via an API if needed for more creativity). For now, possibly implement them in a simplistic way (e.g., generateQuiz could pull 5 random questions from the content library or have a static list for demo).
    • Provide resources like class performance data by querying the DB (we might need to aggregate scores from assignments table, etc. – implement a DB query for that).
    • Prompt template for the teacher persona. (Owner: Similar approach with Cloud generating skeleton functions and human completing the data queries. We'll also need to create some sample data for class performance to test.)*
    • Task 2.4: Teacher Front-end Integration – Build UI features for teachers:
    • Develop a teacher dashboard view where the AI's responses can be shown in different formats. For example, if the teacher asks for a report or generated quiz, the response might be multi-line or contain structured data. The UI should present it cleanly (maybe monospaced or formatted text).
    • Provide controls for teacher to select context if needed (e.g., which class they are asking about if they teach multiple, or filters for performance reports).
    • Possibly a component to preview quiz questions or lesson plan content nicely (maybe allow the teacher to save or edit them). (Owner: Cloud to assist with React components for teacher dashboard and any modals for content, human to refine styling and state management.)*
    • Task 2.5: Admin MCP Server Implementation – Develop the Admin server:
    • Implement admin tools like generateReport, manageContent, etc. These will involve various parts of the database. For example, generateReport might run some SQL aggregate queries for the given timeframe. We might use Supabase's built-in functions or write some logic in the server to compute stats.
    • Implement user management actions carefully: these functions should check that the admin user has privileges and then perform the action (e.g., toggling a user's active status in the DB). We will include confirmation steps where appropriate (the AI can ask user to confirm before calling them).
    • Prepare prompt template for admin persona. (Owner: Cloud for initial code. Human to add safety checks and possibly simulate some reports via sample data.)*
    • Task 2.6: Admin Front-end Integration – Build UI for admin:
    • An admin console page where the admin can chat with their AI assistant. Given the likely complexity of outputs (reports, tables), ensure the chat view can handle these (maybe use markdown rendering for tables or have the AI respond in Markdown which the front-end can render).
    • Add UI components for any interactive admin actions. For instance, if the AI says "Found 5 feedback items" we might allow the admin to click to view details. Or for user management, the AI could produce a clickable link to the user profile in the admin UI. Designing these interactions might require iterating on how the AI formats responses (we could include some special markdown or JSON in answers that the front-end interprets).
    • Basic admin analytics dashboards (non-AI) may be provided too, but since the AI can give data on request, we might keep the UI minimal. (Owner: Cloud to outline a simple admin dashboard page. Human to connect it with any needed API routes.)
We will iterate through Tasks 2.1 to 2.6, testing each role's features thoroughly before moving to the next. We can overlap some tasks (e.g., while Cloud works on 2.3, a developer could be refining 2.2).
Milestones: - After student features (2.1, 2.2) are done, we should have a demo where a student user can log in, ask a homework question, get an answer referencing their context, and maybe try a practice quiz. - After teacher features, a teacher user should successfully generate a quiz and see class insights via chat. - After admin features, an admin should retrieve a system report and perform a management action via the AI.
Phase 3: Refinement, Testing, and Hardening
Once core features are in place, we allocate time to polish and ensure the system meets quality standards.
    • Task 3.1: Comprehensive Testing – Write and run tests:
    • Unit tests for each MCP server tool function (e.g., test that generateQuiz returns the expected number of questions, manageContent correctly updates a course status, etc.). Use a test database or mocking for these tests to not affect real data.
    • Integration tests simulating user flows: e.g., simulate a student session asking a question that triggers a tool usage, verify the final answer is formatted correctly and the tool was invoked. For this, we might use a mock LLM (one that just echoes tool calls) to test the loop.
    • Security tests: attempt cross-role access (simulate a student session trying to call a teacher tool) and ensure it's blocked.
    • Performance tests: measure response times for a typical query to see if any optimization is needed (like caching frequent data or adjusting prompt size). (Owner: Human primarily, though Cloud can help write test cases code.)
    • Task 3.2: UI/UX Improvements – Refine the front-end:
    • Ensure the chat interface is smooth (auto-scroll, loading indicators when waiting for AI response, error messages if something fails).
    • Polish formatting of AI outputs: maybe the AI can use Markdown for lists or tables which we render, ensure code blocks or formulas (if any) display nicely.
    • Add any help text or tutorial elements for users (especially for first-time users to know how to interact with their AI assistant effectively).
    • Cross-browser testing and responsive design checks for the UI.
    • Task 3.3: Prompt Tuning – Improve the AI's responses by adjusting prompt templates or adding few-shot examples:
    • Observe outputs during testing; if the student AI sometimes gives too advanced an answer, tweak the student prompt to simplify language further.
    • Add a few example interactions in the system prompt for each role to guide style (for instance, an example of the teacher AI creating a lesson plan in a certain format).
    • Implement guardrails if needed (like adding "If you're not sure, say you'll get back" or restricting the AI from certain content if it's not allowed).
    • Possibly integrate an AI moderation step for student AI to filter any inappropriate content (OpenAI API has moderation, or we can implement a simple keyword filter).
    • Task 3.4: Documentation & CloudMD – Update documentation:
    • Write a README or Developer Guide for the project explaining how to run it, how the architecture is organized, etc.
    • Update the CloudMD file with any changes in spec or new guidelines discovered during development, so it remains a useful reference for future AI contributions.
    • Prepare user guides for each role if needed (or at least document features for stakeholders).
    • Ensure our spec documents (overview, architecture, etc.) are up to date with the implementation (the spec is living documentation; if anything changed, reflect it now).
    • Task 3.5: Final Review and UAT – Have end-to-end run-throughs:
    • For each role, have a sample user try using the system for typical tasks. For instance, a mock student tries to learn a topic with the AI, a mock teacher generates some materials, an admin checks stats.
    • Gather feedback on any issues or missing features. Fix any critical bugs or usability problems discovered.
    • Verify that the acceptance criteria for all user stories are met or see if any story was partially addressed and decide if it's okay for v1 or needs completion.
Milestone: At the end of Phase 3, the platform should be fully functional and polished for a v1 release, covering all major scenarios. All user stories in epics 1-3 should be demonstrably satisfied. We should also have confidence in the system's reliability and security.
Phase 4: Deployment and Future Work
Finally, when we're ready to go live: - Task 4.1: Deployment Setup – Containerize the application or set up hosting environment. Possibly: - Deploy the front-end to a platform like Vercel or Netlify (if Next.js, could be Vercel). - Deploy the backend to a Node hosting environment or as serverless functions. (Supabase could host a small Node function if using Edge Functions, or we use a small VM.) - Ensure environment variables (API keys, database URL) are configured securely. - Migrate the database schema to production Supabase project and transfer any necessary seed data. - Task 4.2: Monitoring & Logging – Implement basic monitoring: - Use Supabase logs or set up logging in our Node backend to track errors or important events. - Possibly integrate an admin tool to view conversation logs (for moderation) or at least log them server-side in case of issues. - Set up alerts for critical failures (if the AI API is down, etc.). - Task 4.3: Future Roadmap Planning – Identify nice-to-have features and improvements not done in v1: - e.g., Multi-language support (if needed, to have the AI respond in other languages for language courses). - More advanced adaptivity in the student tutor (like detecting frustration or boredom via sentiment). - Additional MCP servers like a "Parent dashboard" or "Peer tutoring" agent. - Integration with external systems (like school LMS integration to fetch real-time grades). - Upgrading the AI model or adding multimodal support (imagine if students could upload an image of their work for AI feedback). - This roadmap helps focus the next phases after initial release.
The tasks above are listed in a logical order, but in practice we may adjust the sequence slightly to balance workload (for example, while waiting for API access we might do more front-end work). The important aspect is we have covered all aspects: front-end, back-end, AI integration, testing, and even the methodology of using AI to build AI.
Throughout development, ChannelMD will be used to coordinate concurrent tasks by multiple AI agents or developers. For example, when working on Phase 2:
[Cloud-Agent1] (in channel.md): Working on StudentServer tools implementation (Task 2.1). Expect completion in 15 minutes.
[Human PM] (in channel.md): Noted. Cloud-Agent2, please start on Student Front-end (Task 2.2) concurrently.
[Cloud-Agent2]: Setting up StudentChat React component and API integration.
...
[Cloud-Agent1]: Completed StudentServer with functions searchCourseContent, getPracticeQuiz, etc. Needs review.
[Human PM]: Reviewing Cloud-Agent1's output... Found minor issues in query logic, will fix. Meanwhile, Cloud-Agent1 can proceed to TeacherServer (Task 2.3).
[Cloud-Agent2]: Student front-end done (basic version). Posting screenshot of UI for review.
[Human PM]: Looks good. Please ensure it handles loading states. Next, Cloud-Agent2, start on Teacher front-end after Cloud-Agent1 finishes TeacherServer.
This is an illustrative snippet showing how we imagine the parallel workflow. The project manager (human) oversees this channel, ensuring agents collaborate effectively. The channel log also serves as documentation of what was done by whom.
Finally, by following this plan, we ensure no relevant feature or detail is omitted. The spec and plan above give a complete blueprint of the project's scope, architecture, and execution strategy. All stakeholders – developers, AI agents, and the project manager – can refer to this spec as the single source of truth while building the AI-powered learning platform.

[1] [2] [3] [4] [5] [6] What is Model Context Protocol (MCP): Explained - Composio
https://composio.dev/blog/what-is-model-context-protocol-mcp-explained
